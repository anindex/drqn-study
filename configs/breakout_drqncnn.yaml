env:
  game: 'Breakout-v0'
  stack_len: 2
  seed: 1314121
  solved_criteria: 30
  q_threhold: 100
  pomdp: True
  pomdp_type: 'flickering'  # flickering|delete_dim
  # pomdp_mask: [1, 0, 1, 0]  # this mask is chosen depended on the hiding dimensions and the observation shape of the env (i.e. 0)
  pomdp_prob: 0.7
  mode: 0

model:
  model_file: 'breakout_dqncnn.pt'
  kernel_num: 16
  hidden_dim: 512
  enable_dueling: False
  dueling_type: 'avg'
  dtype: 'torch.float32'

memory:
  size: 100000
  a: 0.6
  b: 0.4

env_type: 'atari'
model_type: 'drqn_cnn'
memory_type: 'random'
agent_type: 'drqn'
mode: 'train'  # train|test
value_criteria: 'smooth_l1_loss'
optimizer: 'Adam'
train_visualize: False
retrain: True
solved_stop: True

random_eps: 0
episodes: 100000
steps: 22000000  # max possible steps
drqn_n_step: 3
learn_start: 5000
gamma: 0.99
clip_grad: 1.0
lr: 0.001
lr_decay: False
eps_start: 1.0
eps_decay: 200000
eps_end: 0.05
prog_freq: 1000
target_model_update: 1000
batch_size: 32
train_interval: 1
log_window_size: 100
test_nepisodes: 1
log_step_interval: 100
log_episode_interval: 10
use_tensorboard: False
bootstrap_type: 'double_q'  # learn_q|target_q|double_q